import streamlit as st
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Optional: SHAP
try:
    import shap
    SHAP_AVAILABLE = True
except Exception:
    SHAP_AVAILABLE = False

# Optional: TensorFlow / Keras for LSTM
try:
    import tensorflow as tf
    tf.config.run_functions_eagerly(False)
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
    TF_AVAILABLE = True
except Exception:
    TF_AVAILABLE = False

st.set_page_config(page_title="Solar Energy XAI (RF + LSTM)", page_icon="‚òÄÔ∏è", layout="wide")

st.markdown("""
    <style>
    .main-header { font-size: 1.8rem; color: #FF6B35; text-align: center; margin-bottom: 1rem; }
    .sub-header { font-size: 1.1rem; color: #004E89; margin-top: 1rem; }
    .metric-card { background-color: #f0f2f6; padding: 1rem; border-radius: 0.5rem; margin: 0.5rem 0; }
    </style>
""", unsafe_allow_html=True)

st.markdown('<p class="main-header">‚òÄÔ∏è INTEGRASI EXPLAINABLE AI ‚Äî RANDOM FOREST & LSTM</p>', unsafe_allow_html=True)

st.info("""
    ‚ö†Ô∏è **Peringatan Alur Kerja Baru**: Tune/Train Models (Tab 2) ‚Üí Bandingkan Model (Tab 3) ‚Üí Gunakan XAI (Tab 4) ‚Üí Predict (Tab 5).
""")

# ------------ Utilities (Same as before) ------------
# (omitted for brevity, assume generate_sample_data, reset_tensorflow_session, create_lstm_sequences, evaluate_regression, display_metrics_card are here)

# Re-defining required utilities for context
@st.cache_data
def generate_sample_data(n_samples=1200, seed=42):
    np.random.seed(seed)
    dates = pd.date_range(start="2023-01-01", periods=n_samples, freq="15min")
    hours = dates.hour + dates.minute/60
    days = np.arange(n_samples) / 96
    daily_pattern = np.sin((hours - 6) * np.pi / 12)
    seasonal_variation = 1 + 0.3 * np.sin(days * 2 * np.pi / 365)
    base_irradiance = 800 * daily_pattern * seasonal_variation
    irradiance = np.maximum(0, base_irradiance + np.random.normal(0, 50, n_samples))
    irradiance = np.where((hours < 6) | (hours > 18), 0, irradiance)
    ambient = 20 + 10 * np.sin((hours - 8) * np.pi / 12) + np.random.normal(0, 2, n_samples)
    module = ambient + 25 * (irradiance / 1000) + np.random.normal(0, 3, n_samples)
    humidity = np.clip(60 - 20 * np.sin((hours - 8) * np.pi / 12) + np.random.normal(0, 10, n_samples), 20, 95)
    wind = np.abs(5 + 3 * np.sin(hours * np.pi / 6) + np.random.normal(0, 1.5, n_samples))
    pressure = 1013 + np.random.normal(0, 5, n_samples)
    cloud = np.clip(30 + 25 * np.sin(days * 2 * np.pi / 7) + np.random.normal(0, 15, n_samples), 0, 100)
    efficiency = np.clip(18 - 0.004 * (module - 25) + np.random.normal(0, 0.5, n_samples), 12, 22)
    dc_power = (irradiance * 1.6 * efficiency / 100) * (1 - cloud / 200)
    dc_power = np.maximum(0, dc_power + np.random.normal(0, 5, n_samples))
    ac_power = np.maximum(0, dc_power * 0.96 + np.random.normal(0, 3, n_samples))

    df = pd.DataFrame({
        "timestamp": dates,
        "irradiance": irradiance,
        "ambient_temperature": ambient,
        "module_temperature": module,
        "humidity": humidity,
        "wind_speed": wind,
        "atmospheric_pressure": pressure,
        "cloud_cover": cloud,
        "panel_efficiency": efficiency,
        "dc_power": dc_power,
        "ac_power": ac_power
    })
    return df

def reset_tensorflow_session():
    if TF_AVAILABLE:
        try:
            tf.keras.backend.clear_session()
            import gc
            gc.collect()
            return True
        except Exception:
            return False
    return False

def create_lstm_sequences(X, y, seq_len):
    Xs, ys = [], []
    for i in range(len(X) - seq_len):
        Xs.append(X[i:i+seq_len])
        ys.append(y[i+seq_len])
    return np.array(Xs), np.array(ys)

def evaluate_regression(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100
    return rmse, r2, mae, mape 

def display_metrics_card(metrics_tuple, model_name):
    rmse, r2, mae, mape = metrics_tuple
    st.markdown(f"### {model_name} Performance")
    st.metric("R¬≤ Score (Test)", f"{r2:.3f}")
    st.metric("RMSE (Test)", f"{rmse:.2f} W")
    st.metric("MAE (Test)", f"{mae:.2f} W")
    st.metric("MAPE (Test)", f"{mape:.2f}%")

# ------------------------------------------------------------------
# --- NEW: COMBINED HYPERPARAMETER TUNING & TRAINING LOGIC ---
# ------------------------------------------------------------------
def create_lstm_model(n_features, lstm_seq_len, units1=128, units2=64, dropout=0.2, learning_rate=0.001):
    model = Sequential([
        LSTM(units1, input_shape=(lstm_seq_len, n_features), return_sequences=True),
        Dropout(dropout),
        LSTM(units2, return_sequences=False),
        Dropout(dropout),
        Dense(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(1, activation='linear')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])
    return model

def tune_and_train_all_models(df, test_size, rf_params, lstm_params_user):
    FEATURES = ['irradiance','ambient_temperature','module_temperature','humidity','wind_speed','atmospheric_pressure','cloud_cover','panel_efficiency']
    
    with st.status("‚öôÔ∏è Preparing Data and Tuning Models (This may take a moment)...", expanded=True) as status:
        
        # 1. Data Preparation
        X = df[FEATURES].copy()
        y = df['ac_power'].copy()
        
        # Split for RF
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
        scaler_rf = StandardScaler()
        X_train_s = scaler_rf.fit_transform(X_train)
        X_test_s = scaler_rf.transform(X_test)
        
        # Data for LSTM
        scaler_lstm = StandardScaler()
        X_scaled = scaler_lstm.fit_transform(X.values)
        X_seq, y_seq = create_lstm_sequences(X_scaled, y.values, lstm_params_user['seq_len'])
        
        if len(X_seq) < 50:
            status.update(label="‚ùå Training Failed: Not enough sequence samples for LSTM.", state="error")
            st.error("‚ùå Not enough sequence samples. Reduce sequence length.")
            return

        split_idx = int(len(X_seq) * (1 - test_size))
        X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]
        y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]
        
        status.update(label="Data Preparation Complete.", state="running")

        
        # 2. Random Forest Tuning (Grid Search)
        with st.spinner("üå≤ Tuning Random Forest..."):
            
            # Simple Grid Search for RF
            rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)
            grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=rf_params, 
                                        scoring='neg_mean_squared_error', cv=3, verbose=0, n_jobs=-1)
            
            grid_search_rf.fit(X_train_s, y_train)
            
            best_rf = grid_search_rf.best_estimator_
            best_params_rf = grid_search_rf.best_params_
            
            y_pred_train_rf = best_rf.predict(X_train_s)
            y_pred_test_rf = best_rf.predict(X_test_s)
            
            rf_metrics_test = evaluate_regression(y_test, y_pred_test_rf)
            rf_metrics_train = evaluate_regression(y_train, y_pred_train_rf)

            st.session_state['rf_model'] = best_rf
            st.session_state['rf_scaler'] = scaler_rf
            st.session_state['rf_X_test_s'] = X_test_s
            st.session_state['rf_y_test'] = y_test
            st.session_state['rf_y_pred'] = y_pred_test_rf
            st.session_state['rf_metrics'] = rf_metrics_test
            st.session_state['rf_metrics_full'] = {
                'rmse_train': rf_metrics_train[0], 'r2_train': rf_metrics_train[1], 'mae_train': rf_metrics_train[2], 'mape_train': rf_metrics_train[3],
                'rmse_test': rf_metrics_test[0], 'r2_test': rf_metrics_test[1], 'mae_test': rf_metrics_test[2], 'mape_test': rf_metrics_test[3]
            }
            st.session_state['rf_best_params'] = best_params_rf
            st.session_state['rf_trained'] = True

            status.update(label="Random Forest Tuning Complete.", state="running")
        
        # 3. LSTM Training (using best parameters + user-defined epochs)
        with st.spinner("üß† Training LSTM..."):
            if TF_AVAILABLE:
                reset_tensorflow_session()
                
                n_features = X_seq.shape[2]
                lstm_model_tuned = create_lstm_model(
                    n_features=n_features, 
                    lstm_seq_len=lstm_params_user['seq_len'],
                    learning_rate=0.001 # Fixed learning rate for simplicity
                )

                es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

                history = lstm_model_tuned.fit(X_train_seq, y_train_seq,
                                validation_data=(X_test_seq, y_test_seq),
                                epochs=lstm_params_user['epochs'],
                                batch_size=lstm_params_user['batch'],
                                callbacks=[es],
                                verbose=0)
                
                y_pred_train_lstm = lstm_model_tuned.predict(X_train_seq, verbose=0).flatten()
                y_pred_test_lstm = lstm_model_tuned.predict(X_test_seq, verbose=0).flatten()

                lstm_metrics_test = evaluate_regression(y_test_seq, y_pred_test_lstm)
                lstm_metrics_train = evaluate_regression(y_train_seq, y_pred_train_lstm)

                st.session_state['lstm_model'] = lstm_model_tuned
                st.session_state['lstm_scaler'] = scaler_lstm
                st.session_state['lstm_seq_len'] = lstm_params_user['seq_len']
                st.session_state['lstm_X_test_seq'] = X_test_seq
                st.session_state['lstm_y_test'] = y_test_seq
                st.session_state['lstm_y_pred'] = y_pred_test_lstm
                st.session_state['lstm_metrics'] = lstm_metrics_test
                st.session_state['lstm_metrics_full'] = {
                    'rmse_train': lstm_metrics_train[0], 'r2_train': lstm_metrics_train[1], 'mae_train': lstm_metrics_train[2], 'mape_train': lstm_metrics_train[3],
                    'rmse_test': lstm_metrics_test[0], 'r2_test': lstm_metrics_test[1], 'mae_test': lstm_metrics_test[2], 'mape_test': lstm_metrics_test[3]
                }
                st.session_state['lstm_best_params'] = {
                    'epochs': len(history.history['loss']), 
                    'batch_size': lstm_params_user['batch'],
                    'sequence_length': lstm_params_user['seq_len']
                }
                st.session_state['lstm_history'] = history.history
                st.session_state['lstm_trained'] = True
            
            status.update(label="LSTM Training Complete.", state="running")


        if st.session_state.get('rf_trained', False) and st.session_state.get('lstm_trained', False):
            status.update(label="üéâ All Models Tuned and Trained Successfully!", state="complete")
            st.success("üéâ Tuning and Training Complete! See results in Tab 3.")
        else:
            status.update(label="‚ùå Tuning/Training Failed for one or more models.", state="error")

# Display results after combined training
def display_combined_results():
    st.markdown("---")
    st.markdown("## üìä Model Training Results (After Tuning)")
    
    col_rf, col_lstm = st.columns(2)
    
    if st.session_state.get('rf_trained', False):
        with col_rf:
            st.markdown("### üå≤ Random Forest Regressor")
            st.json(st.session_state['rf_best_params'])
            display_metrics_card(st.session_state['rf_metrics'], "")
            
    if st.session_state.get('lstm_trained', False):
        with col_lstm:
            st.markdown("### üß† LSTM Neural Network")
            st.json(st.session_state['lstm_best_params'])
            display_metrics_card(st.session_state['lstm_metrics'], "")
    
    if st.session_state.get('rf_trained', False) or st.session_state.get('lstm_trained', False):
        st.info("Lanjut ke **Tab 3: Model Comparison** untuk detail perbandingan performa.")
        
        if st.session_state.get('lstm_trained', False) and 'lstm_history' in st.session_state:
             # Add LSTM History plot back here
             st.markdown("#### üìâ LSTM Training History")
             # ... (plotting code for LSTM history goes here)

# --- COMBINED SHAP LOGIC ---
def compute_all_xai(model_type, shap_method):
    
    if model_type == 'rf':
        if not st.session_state.get('rf_trained', False):
            st.error("‚ö†Ô∏è Train Random Forest model first.")
            return
        
        with st.spinner("üî¨ Computing SHAP values for Random Forest..."):
            try:
                rf = st.session_state['rf_model']
                X_test_s = st.session_state['rf_X_test_s']
                
                explainer = shap.TreeExplainer(rf)
                shap_values = explainer.shap_values(X_test_s)
                
                st.session_state['rf_shap_values'] = shap_values
                st.session_state['rf_shap_explainer'] = explainer
                
                st.success("‚úÖ SHAP computed for RF")
                return True
            except Exception as e:
                st.error(f"‚ùå SHAP RF error: {e}")
                return False

    elif model_type == 'lstm':
        if not st.session_state.get('lstm_trained', False):
            st.error("‚ö†Ô∏è Train LSTM model first.")
            return
        if not TF_AVAILABLE:
            st.error("‚ö†Ô∏è TensorFlow not available.")
            return

        with st.spinner(f"üî¨ Computing {shap_method} for LSTM..."):
            # (Logic for Permutation Importance or KernelExplainer goes here, same as original Tab 3)
            # ... (Simplified placeholder for brevity) ...
            st.success(f"‚úÖ {shap_method} computed for LSTM (Placeholder)")
            # You would implement the full logic from the original code here
            st.session_state['lstm_perm_importance'] = np.array([0.1, 0.5, 0.2, 0.1, 0.05, 0.05, 0.0, 0.0]) # Dummy data
            return True

# --- Prediction & Display Logic (Same as before) ---
# (omitted predict_all_models and display_prediction_results for brevity)


# ------------ Sidebar config & data load (Same as before) ------------

with st.sidebar:
    # ... (Sidebar content remains the same, assuming df, rf_n_estimators, rf_max_depth, 
    # lstm_seq_len, lstm_epochs, lstm_batch, test_size are defined) ...
    # Placeholder definitions for code flow
    df = generate_sample_data(1200)
    rf_n_estimators = 100
    rf_max_depth = 10
    lstm_seq_len = 24
    lstm_epochs = 20
    lstm_batch = 32
    test_size = 0.2

FEATURES = ['irradiance','ambient_temperature','module_temperature','humidity','wind_speed','atmospheric_pressure','cloud_cover','panel_efficiency']

# Initialize session state (Same as before)
if 'rf_trained' not in st.session_state: st.session_state['rf_trained'] = False
if 'lstm_trained' not in st.session_state: st.session_state['lstm_trained'] = False


# ------------ Tabs UI (New Order) ------------

# Tab order changed: Data, Train, Compare, XAI, Predict
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìä Data & EDA", 
    "‚öôÔ∏è Tune & Train", 
    "‚öñÔ∏è Model Comparison", 
    "üîç XAI (SHAP)", 
    "üìà Predictions"
])

# --- Tab 1: Data & EDA (Same as before) ---
with tab1:
    st.subheader("üìã Data Preview")
    # ... (Full content of Tab 1) ...

# --- Tab 2: Tune & Train (UPDATED with Tuning Button) ---
with tab2:
    st.subheader("‚öôÔ∏è Hyperparameter Tuning and Training")
    
    train_col1, train_col2 = st.columns(2)
    
    with train_col1:
        st.markdown("### üå≤ Random Forest Regressor")
        st.write(f"**Tuning Grid:** n_estimators=[50, 100, 200], max_depth=[5, 10, 20]")
        
    with train_col2:
        st.markdown("### üß† LSTM Neural Network")
        st.write(f"**Train Params:** Sequence={lstm_seq_len}, Epochs={lstm_epochs}, Batch={lstm_batch}")
    
    st.markdown("---")
    
    # Define parameters for Grid Search (Hardcoded for simplicity)
    RF_PARAM_GRID = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 20]
    }
    LSTM_USER_PARAMS = {
        'seq_len': lstm_seq_len,
        'epochs': lstm_epochs,
        'batch': lstm_batch
    }
    
    # Tombol GABUNGAN untuk Tuning & Training
    if st.button("üöÄ Tune Hyperparameters & Train BOTH", key="tune_all_btn", use_container_width=True):
        st.session_state['rf_trained'] = False
        st.session_state['lstm_trained'] = False
        
        tune_and_train_all_models(df, test_size, RF_PARAM_GRID, LSTM_USER_PARAMS)
        
    # Reset TensorFlow Button (Same as before)
    if st.session_state.get('lstm_trained', False) and ('lstm_shap_values' in st.session_state or 'lstm_perm_importance' in st.session_state) and TF_AVAILABLE:
        # ... (Reset TF Button logic)
        pass # Placeholder

    if st.session_state.get('rf_trained', False) or st.session_state.get('lstm_trained', False):
        display_combined_results()

# --- Tab 3: Model Comparison (Now located here) ---
with tab3:
    st.subheader("‚öñÔ∏è Model Comparison")
    # ... (Full content of original Tab 5, using 'rf_metrics_full' and 'lstm_metrics_full') ...
    if not st.session_state.get('rf_trained', False) and not st.session_state.get('lstm_trained', False):
        st.warning("‚ö†Ô∏è Train both models to see comparison")
    else:
        # ... (Comparison visualization and dataframes logic from original Tab 5) ...
        st.info("Displaying comparison after Hyperparameter Tuning.")


# --- Tab 4: XAI (SHAP) (Now located here) ---
with tab4:
    st.subheader("üîç Explainable AI dengan SHAP")
    
    if not SHAP_AVAILABLE:
        st.error("‚ö†Ô∏è SHAP not installed.")
    else:
        st.markdown("### üõ†Ô∏è XAI Configuration")
        
        # User selects model and method
        xai_model = st.selectbox("Pilih Model untuk Analisis XAI", ['Random Forest', 'LSTM'])
        
        if xai_model == 'LSTM':
            st.markdown("#### üß† Explainability Method for LSTM")
            shap_method = st.radio("", ["Permutation Importance ‚≠ê", "KernelExplainer (SHAP)"])
        else:
            shap_method = "TreeExplainer (SHAP)"

        st.markdown("---")
        
        if st.button(f"üî¨ Compute XAI for {xai_model}", key="compute_xai_btn", use_container_width=True):
            if xai_model == 'Random Forest':
                compute_all_xai('rf', shap_method)
            else:
                compute_all_xai('lstm', shap_method)

        # Display XAI Results (Same as before, adapted to unified logic)
        
        # RF Display Logic
        if xai_model == 'Random Forest' and 'rf_shap_values' in st.session_state:
             st.markdown("### üå≤ SHAP for Random Forest Results")
             # ... (SHAP plotting logic for RF from original Tab 3) ...
             
        # LSTM Display Logic
        elif xai_model == 'LSTM' and ('lstm_perm_importance' in st.session_state or 'lstm_shap_values' in st.session_state):
             st.markdown("### üß† Explainability for LSTM Results")
             # ... (Plotting logic for PI/SHAP for LSTM from original Tab 3) ...
             
        
# --- Tab 5: Predictions (Now located here) ---
with tab5:
    st.subheader("üìà Manual Predictions")
    # ... (Full content of original Tab 4, using 'predict_all_models' and 'display_prediction_results') ...
